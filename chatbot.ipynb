{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b76fee27-1076-4585-8f12-97f88adc25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib.request\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bac6582b-122e-488c-acf3-6bea59370306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "58b537b0-5f66-4db4-802d-5790a51cf470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ChatbotData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3ceb9024-3718-424d-8073-2af64310ac47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5290\n",
       "1    3570\n",
       "2    2963\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "33efeddc-7a85-4923-8a67-e90660c9e990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'Q':'text'},inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0d9480d5-a6d1-4fef-bf3c-05be935b209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0a03f54e-92b1-466b-9f37-ec0580010152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.447433\n",
       "1    0.301954\n",
       "2    0.250613\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label']=le.fit_transform(df['label'])\n",
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "91cad26e-c02a-48a6-85c7-cc794acf435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, train_labels=df['text'], df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ff8e4ca-c038-44f6-9e21-faf852c265dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c917700-a648-4463-80e8-965ad9d379c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fc52509d-ceb4-40be-b267-974edfad6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a382adc-6bb9-414d-ab00-61c0873304a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the Roberta tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# Import Roberta pretrained model\n",
    "bert = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "44edf272-ab61-4bc5-ab00-d9ca43f38406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "# Load the DistilBert tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# Import the DistilBert pretrained model\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4ed45a2a-fc3e-4ffe-b50e-7bef810003e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"this is a distil bert model.\",\"data is oil\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88f8c8b6-313a-4bbe-b11f-585e7a80c2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  2003,  1037,  4487, 16643,  2140, 14324,  2944,  1012,\n",
      "           102],\n",
      "        [  101,  2951,  2003,  3514,   102,     0,     0,     0,     0,     0,\n",
      "             0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input=tokenizer(text, padding=True, truncation=True, return_tensors='pt') #tokenization, truncate sentence \n",
    "print(encoded_input)\n",
    "\n",
    "#1 - Actual token\n",
    "#0 - Padded token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6ea994a9-ce6f-4f5f-9ff2-2513300ad39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len=[len(i.split()) for i in train_text]\n",
    "pd.Series(seq_len).hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c43cd1b1-5a7d-4399-afff-b6a6be2e592f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_seq_len=8\n",
    "tokens_train=tokenizer(train_text.tolist(), max_length=max_seq_len, pad_to_max_length=True, truncation=True, return_token_type_ids=False) \n",
    "#[[101, 1045], [2986, 1012]]\n",
    "#\"[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\" but false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "531a00b4-b36a-4a23-999e-8b99139cd673",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq=torch.tensor(tokens_train['input_ids'])\n",
    "train_mask=torch.tensor(tokens_train['attention_mask'])\n",
    "train_y=torch.tensor(train_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2e21df95-4e43-49e4-a389-bdbacdfb8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d409ffac-18d5-4af5-8bdd-3e83dd690b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 1, 1, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "58773150-9ec6-4905-a60a-82e0c572a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "49acffec-39a3-4cd5-b330-ffc40ddcb8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "# wrap tensors\n",
    "train_data=TensorDataset(train_seq, train_mask, train_y) \n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler=RandomSampler(train_data) \n",
    "#given probabilities (weights)\n",
    "#([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n",
    "#[0, 1, 4, 3, 2]\n",
    "\n",
    "train_dataloader=DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "708e5ebe-2764-447c-9dee-fc41d29d23a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        \n",
    "        self.bert=bert\n",
    "        \n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "        \n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "        self.fc1=nn.Linear(768,512)\n",
    "        self.fc2=nn.Linear(512,256)\n",
    "        self.fc3=nn.Linear(256, 3)\n",
    "        \n",
    "        self.softmax=nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, sent_id, mask):\n",
    "        cls_hs=self.bert(sent_id, attention_mask=mask)[0][:,0]\n",
    "        \n",
    "        x=self.fc1(cls_hs)\n",
    "        x=self.relu(x)\n",
    "        x=self.dropout(x)\n",
    "        \n",
    "        x=self.fc2(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.dropout(x)\n",
    "        \n",
    "        x=self.fc3(x)\n",
    "        \n",
    "        x=self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a52e1c0c-09d8-4437-9b9c-0aea9b9400d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad=False\n",
    "    \n",
    "model=BERT_Arch(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ffd7a19b-9c8c-4a8d-98cc-e371962df87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BERT_Arch                                               --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              (23,440,896)\n",
       "│    │    └─Embedding: 3-2                              (393,216)\n",
       "│    │    └─LayerNorm: 3-3                              (1,536)\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             (42,527,232)\n",
       "├─Dropout: 1-2                                          --\n",
       "├─ReLU: 1-3                                             --\n",
       "├─Linear: 1-4                                           393,728\n",
       "├─Linear: 1-5                                           131,328\n",
       "├─Linear: 1-6                                           771\n",
       "├─LogSoftmax: 1-7                                       --\n",
       "================================================================================\n",
       "Total params: 66,888,707\n",
       "Trainable params: 525,827\n",
       "Non-trainable params: 66,362,880\n",
       "================================================================================"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a2f762e1-70e2-47fb-bc48-b8d9152c38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "06e8565b-d03c-4089-935e-85ee4d7ee5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer=AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "94e54336-7bf1-4458-ae51-8705c9aa4dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "adf696ab-7edd-449b-8ced-3c9e7ee5080a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2], y=0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "11818    2\n",
      "11819    2\n",
      "11820    2\n",
      "11821    2\n",
      "11822    2\n",
      "Name: label, Length: 11823, dtype: int64 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "class_wts=compute_class_weight('balanced',np.unique(train_labels), train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1679b4a4-0f94-498a-a4ce-5e5d0a9fc72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0a738d20-72a4-43d3-a532-6da6f1edac46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74499055, 1.10392157, 1.33007087])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f2b02a82-946e-485f-9dfc-e26e9fc7c7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7450, 1.1039, 1.3301])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights=torch.tensor(class_wts, dtype=torch.float)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "da3313d2-64d6-4492-9561-447f453382c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLLLoss()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy=nn.NLLLoss(weight=weights)\n",
    "cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8517ff22-f64b-44cd-8926-8327fc520129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d9a7a3cc-3db2-4135-b715-1ac36db28088",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "epochs=2\n",
    "lr_sch=optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fd622a10-a586-49c4-b239-e896c8626f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    \n",
    "    total_preds=[]\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step%100==0 and not step==0:\n",
    "            print('batch {:>5,} of {:>5,}.'.format(step,len(train_dataloader)))\n",
    "        \n",
    "        batch=[r for r in batch]\n",
    "        sent_id, mask, labels=batch \n",
    "        \n",
    "        preds=model(sent_id, mask)\n",
    "\n",
    "        \n",
    "        loss=cross_entropy(preds, labels) #labels=y\n",
    "        \n",
    "        total_loss=total_loss+loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "#         lr_sch.step()\n",
    "\n",
    "        preds=preds.detach().numpy()\n",
    "        total_preds.append(preds)\n",
    "        \n",
    "    avg_loss=total_loss/len(train_dataloader)\n",
    "\n",
    "    total_preds=np.concatenate(total_preds, axis=0)\n",
    "    \n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "96dc45ea-0f0c-4d3e-b27f-c83f4a87880e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1183"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a68c8087-399d-4585-87d7-87ce6bceb3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f90d7e16f40>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ea765a33-a194-4906-9000-0d706e0e49f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/2\n",
      "batch   100 of 1,183.\n",
      "batch   200 of 1,183.\n",
      "batch   300 of 1,183.\n",
      "batch   400 of 1,183.\n",
      "batch   500 of 1,183.\n",
      "batch   600 of 1,183.\n",
      "batch   700 of 1,183.\n",
      "batch   800 of 1,183.\n",
      "batch   900 of 1,183.\n",
      "batch 1,000 of 1,183.\n",
      "batch 1,100 of 1,183.\n",
      "\n",
      " Epoch 2/2\n",
      "batch   100 of 1,183.\n",
      "batch   200 of 1,183.\n",
      "batch   300 of 1,183.\n",
      "batch   400 of 1,183.\n",
      "batch   500 of 1,183.\n",
      "batch   600 of 1,183.\n",
      "batch   700 of 1,183.\n",
      "batch   800 of 1,183.\n",
      "batch   900 of 1,183.\n",
      "batch 1,000 of 1,183.\n",
      "batch 1,100 of 1,183.\n",
      "\n",
      " training loss:0.989\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print('\\n Epoch {}/{}'.format(epoch+1, epochs))\n",
    "    \n",
    "    train_loss, _=train()\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "print(f'\\n training loss:{train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6ed0d471-f0e0-4c80-958f-dbe8129245be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"intents\": [\n",
    "{\"tag\": 0,\n",
    " \"responses\": [\"하루가 또 가네요.\", \"위로해 드립니다.\", \"여행은 언제나 좋죠.\",   \"눈살이 찌푸려지죠.\", \"응원합니다!\"]},\n",
    "{\"tag\": 1,\n",
    " \"responses\": [\"더 오래 만날 사람 만날 거예요.\", \"더 공허함이 크시겠네요.\", \"더 좋은 사람 만나실 거예요.\", \"더 마음이 허하겠어요.\"]},\n",
    "{\"tag\": 2,\n",
    " \"responses\": [\"짝사랑 만큼 감정소모가 큰 건 없을 거예요.\", \"정열적인 사랑을 하고 있나봐요.\",  \"서로 깊게 알게되면서 더 좋아졌나봅니다.\"]}\n",
    "]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91c096fb-d45b-4136-a9fd-a3142b0e817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "264a0050-5452-4cb5-9d79-3aac16caea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(str):\n",
    "    str=re.sub(r'[^a-zA-Z ]+', '', str)\n",
    "    test_text=[str]\n",
    "    model.eval()\n",
    "    \n",
    "    tokens_test_data=tokenizer(test_text, max_length=max_seq_len, pad_to_max_length=True, truncation=True, return_token_type_ids=False)\n",
    "    \n",
    "#     return tokens_test_data\n",
    "\n",
    "    test_seq=torch.tensor(tokens_test_data['input_ids'])\n",
    "    test_mask=torch.tensor(tokens_test_data['attention_mask'])\n",
    "    \n",
    "\n",
    "    preds=None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds=model(test_seq, test_mask)\n",
    "#         preds=preds.squeeze()\n",
    "    preds=preds.detach().numpy()\n",
    "#     print(preds.shape)\n",
    "    preds=np.argmax(preds, axis=1)\n",
    "    print('intent identified: ', le.inverse_transform(preds)[0])\n",
    "    return le.inverse_transform(preds)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8418d07c-f660-407c-8c9a-a30b2a92f2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent identified:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction('하루가 또 가네요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d3661add-f4e8-41d2-b8f9-bf744d3e675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(message): \n",
    "    intent = get_prediction(message)\n",
    "#     result=None\n",
    "    for i in data['intents']: \n",
    "        if i[\"tag\"] == intent:\n",
    "            result = random.choice(i[\"responses\"])\n",
    "            break\n",
    "    print(f\"Response : {result}\")\n",
    "    return intent, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a4c65eb8-fbc3-4ba8-834d-56eaad1d63d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent identified:  1\n",
      "Response : 더 공허함이 크시겠네요.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, '더 공허함이 크시겠네요.')"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response('힘들어서 결혼할까봐')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8b2a85d5-b463-4e75-9f60-1603ea9f1662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "51c912b4-705b-4c39-8249-dc4fe9f69a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 0,\n",
       "   'responses': ['하루가 또 가네요.',\n",
       "    '위로해 드립니다.',\n",
       "    '여행은 언제나 좋죠.',\n",
       "    '눈살이 찌푸려지죠.',\n",
       "    '응원합니다!']},\n",
       "  {'tag': 1,\n",
       "   'responses': ['더 오래 만날 사람 만날 거예요.',\n",
       "    '더 공허함이 크시겠네요.',\n",
       "    '더 좋은 사람 만나실 거예요.',\n",
       "    '더 마음이 허하겠어요.']},\n",
       "  {'tag': 2,\n",
       "   'responses': ['짝사랑 만큼 감정소모가 큰 건 없을 거예요.',\n",
       "    '정열적인 사랑을 하고 있나봐요.',\n",
       "    '서로 깊게 알게되면서 더 좋아졌나봅니다.']}]}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492bc464-40b6-469d-aa2a-48ea089faba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
